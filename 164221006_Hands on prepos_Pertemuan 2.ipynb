{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>Natural Language Processing</strong><br />\n",
    "<strong><font color=\"blue\">Semester Gasal T.A. 2024/2025</font></strong><br />\n",
    "</center>\n",
    "\n",
    "<strong>Outline pertemuan minggu ke-4</strong><br />\n",
    "<li> Tokenisasi </li>\n",
    "<li> Casefolding </li>\n",
    "<li> Stemming dan Lemmatization</li>\n",
    "<li> Part of Speech (POS) tagging </li>\n",
    "<li> Stopword removal </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
    "<p>NLTK dapat melakukan tokenisasi pada level kata dan pada level kalimat</p>\n",
    "\n",
    "<p><strong>Kelebihan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
    "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
    "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Murni Python: relatif lebih lambat</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e', '.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenisasi kata\n",
    "from nltk import word_tokenize\n",
    "\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "word_token = word_tokenize(S)\n",
    "\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'Mr.', 'Man.', 'He', 'smiled!!', 'This,', 'i.e.', 'that,', 'is', 'it.']\n"
     ]
    }
   ],
   "source": [
    "# Mengapa tidak menggunakan fungsi split dari python?? apa bedanya?\n",
    "\n",
    "word_split = S.split()\n",
    "print(word_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban : Split hanya berfokus pada pemecahan kata, sedangkan dalam beberapa analisis lanjutan split tidak dapat menangani secara kompleks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# tokenisasi kalimat\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentence_token = sent_tokenize(S)\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 1:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
    "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
    "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Nafi ?', 'aku Nadila']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Haii !', 'aku Naina']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kerjakan latihan 1 pada cell berikut ini\n",
    "\n",
    "#Jawaban = Iya, tanda \"?\" dan \"!\" dapat memisahkan kalimat\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "contoh1 = \"Hello Nafi ? aku Nadila\"\n",
    "contoh2 = \"Haii ! aku Naina\"\n",
    "\n",
    "sentence_token1 = sent_tokenize(contoh1)\n",
    "sentence_token2 = sent_tokenize(contoh2)\n",
    "print(sentence_token1)\n",
    "sentence_token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Teknologi \\n sains Data']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jawaban = Tidak, tanda enter atau ganti baris tidak dapat memisahkan kalimat\n",
    "\n",
    "contoh3 = \"Teknologi \\n sains Data\"\n",
    "sentence_token3 = sent_tokenize(contoh3)\n",
    "\n",
    "sentence_token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hobiku ; menyanyi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jawaban = Tidak, tanda \";\" tidak dapat memisahkan kalimat\n",
    "\n",
    "contoh4 = \"Hobiku ; menyanyi\"\n",
    "sentence_token4 = sent_tokenize(contoh4)\n",
    "\n",
    "sentence_token4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kamu harus hati - hati']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['co - op']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jawaban = Tidak, tanda hubung tidak dapat memisahkan kalimat\n",
    "\n",
    "contoh5 = \"Kamu harus hati - hati\"\n",
    "sentence_token5 = sent_tokenize(contoh5)\n",
    "\n",
    "print(sentence_token5)\n",
    "\n",
    "contoh6 = \"co - op\"\n",
    "sentence_token6 = sent_tokenize(contoh6)\n",
    "\n",
    "sentence_token6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi dengan modul Spacy\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Di claim lebih cepat (C-based)</li>\n",
    "\t<li>License termasuk untuk komersil</li>\n",
    "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
    "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi kata menggunakan Spacy\n",
    "import spacy\n",
    "\n",
    "# Loading language model\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "S = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "tokens = spacy_en(S)\n",
    "print( [token.text for token in tokens] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi kalimat dengan Spacy\n",
    "sentence_tokens = spacy_en(S).sents\n",
    "print([str(sent) for sent in sentence_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 2:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
    "\t<li>Lakukan latihan seperti yang dilakukan sebelumnya dengan modul NLTK, apakah hasilnya sama dengan Spacy?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi Spacy juga berbeda dengan NLTK.<br />\n",
    "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 2 pada cell berikut ini\n",
    "\n",
    "# Tidak, terdapat perbedaan dalam hasil tokenisasi dan dibuktikan pada percobaan di bawah ini\n",
    "sentence_tokens = spacy_en(S).sents\n",
    "print([str(sent) for sent in sentence_tokens])\n",
    "\n",
    "print(sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perbedaan dari hasil di atas adalah\n",
    "NLTK Cenderung menggabungkan teks yang mungkin dianggap sebagai satu kalimat, sedangkan spacy lebih detail dan bisa memisahkan kalimat berdasarkan analisis konteks dan tanda baca yang lebih rumit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Nafi ?', 'aku', 'Nadila']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens1 = spacy_en(contoh1).sents\n",
    "print([str(sent) for sent in sentence_tokens1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Haii !', 'aku Naina']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens2 = spacy_en(contoh2).sents\n",
    "print([str(sent) for sent in sentence_tokens2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Teknologi \\n sains Data']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens3 = spacy_en(contoh3).sents\n",
    "print([str(sent) for sent in sentence_tokens3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hobiku ; menyanyi']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens4 = spacy_en(contoh4).sents\n",
    "print([str(sent) for sent in sentence_tokens4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kamu harus hati - hati']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens5 = spacy_en(contoh5).sents\n",
    "print([str(sent) for sent in sentence_tokens5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['co - op']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens6 = spacy_en(contoh6).sents\n",
    "print([str(sent) for sent in sentence_tokens6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kesimpulan : Percobaan di atas memiliki hasil yang sama dengan menggunakan NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi dengan TextBlob\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
    "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
    "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
    "\t<li>Language Model terbatas: English, German, French</li>\n",
    "</ol>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi dengan TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "sentence_tokens_blob = TextBlob(T).sentences\n",
    "\n",
    "# Tokenisasi kata\n",
    "print(TextBlob(T).words)\n",
    "\n",
    "# Tokenisasi kalimat\n",
    "print([str(sent) for sent in sentence_tokens_blob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 3:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
    "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e. that, is it.']\n",
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it.']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 3 pada cell berikut ini\n",
    "print(sentence_token)\n",
    "sentence_tokens = spacy_en(S).sents\n",
    "print([str(sent) for sent in sentence_tokens])\n",
    "print([str(sent) for sent in sentence_tokens_blob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK memiliki hasil yang sama dengan textblob. Maka dapat disimpulkan Spacy lebih baik dalam mengenali berdasarkan konteks. \n",
    "Lebih baik atau tidak tergantung dengan konteks yang dibutuhkan dalam analisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi: language dependent dan environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bagaimana untuk data yang punya karakteristik tertentu seperti Twitter?__\n",
    "__Apakah bisa menggunakan modul tokenisasi yang telah tersedia?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'so', 'happpy', ',', 'supeeer', 'happpy', ':)', '#imsohappy', '#happy']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenizer untuk twitter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = \"@stki I am so happpppppppy, supeeeer happpy :) #imsohappy #happy\"\n",
    "print(Tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@stki I am so happy\n"
     ]
    }
   ],
   "source": [
    "import itertools \n",
    "\n",
    "tweet = \"@stki I am so happpppppppppy\"\n",
    "\n",
    "# Menghilangkan double karakter\n",
    "tweet_clear = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "print(tweet_clear)\n",
    "\n",
    "# NOTES: untuk beberapa data spesifik seperti data bioinformatics, cryptography,\n",
    "# twitter, dst dibutuhkan tokenizer custom untuk dapat memenuhi kebutuha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "\n",
    "# load language model bahasa Indonesia\n",
    "spacy_id = Indonesian()\n",
    "\n",
    "S = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "word_token = spacy_id(S)\n",
    "print([token.text for token in word_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "word_token_en = spacy_en(S)\n",
    "print([token.text for token in word_token_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 4:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah ada perbedaan apabila menggunakan language model yang berbeda?</li>\n",
    "    <li>Bagaimana jika melakukan tokenisasi Bahasa Indonesia dengan NLTK? Apakah hasilnya sama dengan Spacy?\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban : Ada, dari percobaan di atas didapatkan bahwa dengan Bahasa Indonesia pemecahan kata lebih baik dan sesuai dengan kata aslinya seperti Kupu-kupu. Sedangkan dengan Bahasa Inggris hanya dilakukan pemisahan tanpa memerhatikan arti atau konteksnya. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban 2 : Tidak bisa, NLTK tidak bisa melakukan tokenisasi dengan memilih bahasanya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerjakan Latihan 4 pada cell berikut ini\n",
    "#word_token2 = word_tokenize(S, language='indonesian')\n",
    "#print(word_token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casefolding\n",
    "\n",
    "<p> Casefolding dilakukan untuk merubah karakter ke dalam huruf besar (uppercase) atau huruf kecil (lowercase) </p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "    <li>Casefolding dapat dilakukan dengan efisien tanpa melalui proses tokenisasi</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there!, i am a student. nice to meet you :)\n",
      "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
     ]
    }
   ],
   "source": [
    "# Contoh casefolding\n",
    "S = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(S.lower())\n",
    "print(S.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 5:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Temukan minimal 2 pengecualian dimana huruf besar dan kecil mempengaruhi makna dalam pemrosesan teks</li>\n",
    "    <li>Mengapa casefolding dapat dilakukan secara efisien tanpa melalui tahap tokenisasi?</li>\n",
    "    <li>Berikan contoh pengaruh dari urutan proses dalam preprocessing yang berpengaruh terhadap hasil preprocessing </li>\n",
    "        \n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Huruf besar kecil memengaruhi makna ketika :\n",
    "1. Terdapat nama orang terutama yang memiliki arti yang sama dengan kata yang lainnya apa bila tidak dibedakan dengan kapitalisasi. Contoh sederhananya adalah nama Sari yang memiliki arti yang sama dengan sari yang berada pada kata benang sari.\n",
    "2. Singkatan atau akronim, contohnya adalah BIN yang memiliki arti Badan Inetelijen Negara, namun tanpa kapitalisasi menjadi arti yang lain.\n",
    "\n",
    "> Casefolding dapat dilakukan tanpa proses tokenisasi karena pada dasarnya, casefolding hanya mengubah keseragaman karakteristik dari teks menjadi sama (upper atau lowercase). Proses ini tidak membutuhkan pemecahan kalimat atau kata karena hanya akan mengubah bentuknya saja.\n",
    "\n",
    "> Contoh pengaruhnya adalah ketika melakukan stop words dan stemming terlebih dahulu, kemudian tokenisasi dan sebaliknya. Proses ini akan dibuktikan di bawah ini.\n",
    "Terdapat perbedaan hasil akhri ketika urutan prepro dibedakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil: ['kue', 'meja', ',', 'kue', 'lezat', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"Ada banyak kue di atas meja, dan kue itu sangat lezat.\"\n",
    "\n",
    "# 1. Stopwords Removal\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# 2. Stemming\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# 3. Tokenisasi\n",
    "final_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Hasil:\", final_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil: ['kue', 'meja', 'kue', 'lezat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import string\n",
    "\n",
    "# Inisialisasi stemmer untuk bahasa Indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "teks = \"Ada banyak kue di atas meja, dan kue itu sangat lezat.\"\n",
    "\n",
    "# Tokenisasi\n",
    "tokens = word_tokenize(teks)\n",
    "\n",
    "# Stopwords Removal\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "# Stemming\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Hasil:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming dan Lemmatization\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemming tidak perlu \"benar\", hanya perlu konsisten__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  presumably I would like to MultiPly my provision, saying tHat without crYing\n",
      "Lancaster :  presum i would lik to multiply my provision, say that without cry\n",
      "Porter :  presum i would like to multipli my provision, say that without cri\n",
      "SnowBall :  presum i would like to multipli my provision, say that without cri\n"
     ]
    }
   ],
   "source": [
    "# Contoh Stemming di NLTK\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "S = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
    "print('Sentence: ',S)\n",
    "\n",
    "stemmer_list = [LancasterStemmer, PorterStemmer, SnowballStemmer] \n",
    "names = ['Lancaster', 'Porter', 'SnowBall']\n",
    "for stemmer_name,stem in zip(names,stemmer_list):\n",
    "    if stemmer_name == 'SnowBall':\n",
    "        st = stem('english')\n",
    "    else:\n",
    "        st = stem()\n",
    "    print(stemmer_name,': ',' '.join(st.stem(s) for s in S.split()))\n",
    "# perhatikan, kita tidak melakukan case normalization (lowercase) \n",
    "# Hasil stemming bisa tidak bermakna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  apples and Oranges are similar. boots and hippos aren't, don't you think?\n",
      "Lemmatize:  apple and Oranges are similar. boot and hippo aren't, don't you think?\n"
     ]
    }
   ],
   "source": [
    "# Contoh Lemmatizer di NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "S = \"apples and Oranges are similar. boots and hippos aren't, don't you think?\"\n",
    "print('Sentence: ', S)\n",
    "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(s) for s in S.split()))\n",
    "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize:  go\n",
      "Lemmatize:  went\n",
      "Lemmatize:  went\n",
      "Lemmatize:  went\n"
     ]
    }
   ],
   "source": [
    "# Contoh TextBlob Stemming & Lemmatizer\n",
    "from textblob import Word\n",
    "# Stemming\n",
    "#print(\"Stem: \", Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
    "\n",
    "# Lemmatizer\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('v'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('n'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('a'))\n",
    "print(\"Lemmatize: \", Word('went').lemmatize('r'))\n",
    "\n",
    "\n",
    "# default Noun, plural akan menjadi singular dari akar katanya\n",
    "# Juga case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'playing', 'better', 'than', 'me']\n",
      "['you', 'are', 'playing', 'better', 'than', 'me']\n",
      "['you', 'be', 'play', 'better', 'than', 'me']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob,Word\n",
    "sentence=\"you are playing better than me\"\n",
    "w=sentence.split(\" \")\n",
    "print(w)\n",
    "print([Word(word).lemmatize() for word in w])\n",
    "print([Word(word).lemmatize(\"v\") for word in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be sure apple and orange be similar . boot and hippos be not , do not you think ?\n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer English\n",
    "sent = \"I am sure Apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
    "sent_token = spacy_en(sent)\n",
    "print( ' '.join( s.lemma_ for s in sent_token ) )\n",
    "# HATI-HATI dengan lemma \"I\" dan \"you\" di Spacy!!!..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer Indonesia\n",
    "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Jogjakarta\"\n",
    "idn = spacy_id(I)\n",
    "print( ' '.join( k.lemma_ for k in idn ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '']\n"
     ]
    }
   ],
   "source": [
    "# Perhatikan output berikut (hati-hati inkonsistensi)\n",
    "print([k.lemma_ for k in spacy_id(\"Perayaan Bepergian\")])\n",
    "\n",
    "# bagaimana dengan Spacy stemmer??\n",
    "# Spacy belum support stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban = Belum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu bareng dengan saat kita pergi ke makassar\n",
      "raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu berbarengan dengan saat kita bepergian ke Makassar\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
    "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
    "# Apa sajakah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jawaban : fokus bahasa pada satrawi adalah bahasa Indonesia jadi tidak perlu melakukan pengubahan bahasa lagi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Tips:\">Tips:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
    "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
    "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
    "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
    "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
    "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) tag\n",
    "\n",
    "<p> POS tagging merupakan proses pemberian tanda berupa kelas kata pada setiap kata yang terdapat di dalam corpus.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"figures/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
    "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# Contoh POS tags dengan NLTK (bahasa Inggris)\n",
    "from nltk import pos_tag\n",
    "S = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "tokens = word_tokenize(S)\n",
    "print(pos_tag(tokens))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daftar POS tag NLTK:\n",
    "\n",
    "<img alt=\"\" src=\"figures/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
    "\n",
    "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('currently', 'RB'),\n",
       " ('learning', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('English', 'NNP'),\n",
       " ('but', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('possible', 'JJ'),\n",
       " ('I', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('know', 'VB'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Indonesian', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('too', 'RB')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh POS tag dengan TextBlob pada bahasa Inggris\n",
    "blob = TextBlob(S)\n",
    "blob.tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello UH, , ,, Mr. NNP, Man NNP, . ., He PRP, smiled VBD, ! ., ! ., This DT, , ,, i.e. FW, that IN, , ,, is VBZ, it PRP, . ., "
     ]
    }
   ],
   "source": [
    "# Contoh POS tag dengan Spacy pada bahasa Inggris\n",
    "tokens = spacy_en(T)\n",
    "for token in tokens:\n",
    "    print(token,token.tag_, end =', ')\n",
    "    \n",
    "# Spacy belum support POS tag untuk bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adverb'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untuk mengetahui arti dari POS tag pada Spacy dapat menggunakan perintah \"explain\"\n",
    "spacy.explain('RB')\n",
    "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 6:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Kapan harus melakukan POS tagging pada tahap preprocessing?</li>\n",
    "    <li>Buatlah contoh hasi dari POS tag dengan hanya mengambil kata yang memiliki tag NOUN (*), dan berikan contoh kasus penggunaannya?</li>\n",
    "    <li>Buatlah contoh hasil dari POS tag yang telah ditambahkan pada setiap kata dalam suatu kalimat dengan menggunakan NLTK (**)</li>\n",
    "        \n",
    "</ul>\n",
    "\n",
    "<p>(*) <strong>Input</strong>: \"The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar.\"</p>\n",
    "<p>(**)</p> \n",
    "<p> <strong>Input</strong>: \"I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too\"</p>\n",
    "<p> <strong>Expected output</strong>: \"I_PRP am_VBP currently_RB learning_VBG NLP_NNP in_IN English_NNP ,_, but_CC if_IN possible_JJ I_PRP want_VBP to_TO know_VB NLP_NNP in_IN Indonesian_JJ language_NN too_RB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pos Tagging dilakukan ketika membutuhkan pemahaman mengenai struktur dasar kata yang kemudian diproses dengan mendapatkan pengetahuan konteksnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: ['tiger', 'cat', 'species', 'member', 'stripes', 'fur', 'underside', 'apex', 'predator', 'ungulates', 'deer', 'boar']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan nomor 6 pada cell ini\n",
    "# (*)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "S1 = 'The tiger (Panthera tigris) is the largest extant cat species and a member of the genus Panthera. It is most recognisable for its dark vertical stripes on orange-brown fur with a lighter underside. It is an apex predator, primarily preying on ungulates such as deer and wild boar'\n",
    "\n",
    "doc = nlp(S1)\n",
    "\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "\n",
    "print(\"Nouns:\", nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# (**)\n",
    "\n",
    "from nltk import pos_tag\n",
    "S2 = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "tokens_6 = word_tokenize(S2)\n",
    "print(pos_tag(tokens_6))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "\n",
    "<p> Stopword removal merupakan salah satu cara untuk melakukan normalisasi pada level kata </p>\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n"
     ]
    }
   ],
   "source": [
    "# Contoh stopword dari NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stw_en = stopwords.words('english')\n",
    "print(nltk_stw_en[:10])\n",
    "\n",
    "nltk_stw_id = stopwords.words('indonesian')\n",
    "print(nltk_stw_id[:10])\n",
    "\n",
    "# Lsit stopword dapat ditambahkan dan dikurangi sesuai dengan kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n"
     ]
    }
   ],
   "source": [
    "#contoh stopword dari Sastrawi pada bahasa Indonesia\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "sastrawi_stw_id = factory.get_stop_words()\n",
    "print(sastrawi_stw_id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips:\n",
    "# selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
    "nltk_stw_en = set(nltk_stw_en)\n",
    "nltk_stw_id = set(nltk_stw_id)\n",
    "sastrawi_stw_id = set(sastrawi_stw_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Latihan:\"><font color=\"blue\">Latihan 7:</font></h3>\n",
    "\n",
    "<p> Lakukan stopword removal pada contoh paragraf berikut ini: </p>\n",
    "<p> \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\" </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siti', 'Nurbaya', 'novel', 'Indonesia', 'ditulis', 'Marah', 'Rusli', 'Novel', 'diterbitkan', 'Balai', 'Pustaka', 'penerbit', 'nasional', 'negeri', 'Hindia', 'Belanda', '1922']\n"
     ]
    }
   ],
   "source": [
    "# Kerjakan latihan 7 pada cell berikut \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import string\n",
    "\n",
    "teks7 = \"Siti Nurbaya adalah sebuah novel Indonesia yang ditulis oleh Marah Rusli. Novel ini diterbitkan oleh Balai Pustaka, penerbit nasional negeri Hindia Belanda, pada tahun 1922.\"\n",
    "\n",
    "tokens_7 = word_tokenize(teks7)\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "filtered_tokens = [word for word in tokens_7 if word.lower() not in stop_words]\n",
    "\n",
    "filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
    "\n",
    "print(filtered_tokens)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
